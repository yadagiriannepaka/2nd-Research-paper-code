{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ad75e9-b64f-4d12-85cc-0699e3a6511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e60295-a78a-4702-9fdd-2fbd33ce3962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California is the biggest example of city plan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A life without cars sure sounds like a great i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cars are used used in everyday life but they m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The electoral collage should be disbanded. Not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Mr.Mrs. Senator, The Electoral College is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>Dear Florida senator, It is in the interest of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>Driving can be a huge hassle. Having to spend ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>Dear\\nSenator,\\n\\nI am writing to express my o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>The electoral college is pointless, I believe ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>Although the usage of motorized vehicles such ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2494 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     California is the biggest example of city plan...      1\n",
       "1     A life without cars sure sounds like a great i...      0\n",
       "2     Cars are used used in everyday life but they m...      0\n",
       "3     The electoral collage should be disbanded. Not...      0\n",
       "4     Dear Mr.Mrs. Senator, The Electoral College is...      0\n",
       "...                                                 ...    ...\n",
       "2489  Dear Florida senator, It is in the interest of...      0\n",
       "2490  Driving can be a huge hassle. Having to spend ...      0\n",
       "2491  Dear\\nSenator,\\n\\nI am writing to express my o...      1\n",
       "2492  The electoral college is pointless, I believe ...      0\n",
       "2493  Although the usage of motorized vehicles such ...      0\n",
       "\n",
       "[2494 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv(\"/home/yadagiri/Downloads/GYANDEEP 2ND PAPER CODE/kaggletestdata1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89df6a1a-c1b7-4837-9c56-d2d073482e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2494, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b706b5-bd8a-490c-9e20-32714ed8ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yadagiri/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/yadagiri/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/home/yadagiri/Downloads/GYANDEEP 2ND PAPER CODE/kaggletestdata1.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "X = list(df['text'])\n",
    "#y = np.asarray(d1[d1.columns[1:]])\n",
    "y_data = df[df.columns[1:2]]\n",
    "y = y_data.values\n",
    "######### Label Encoding for y_train & y_test################\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y\n",
    "# Split dataset into train and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]  # Convert label to integer\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Assuming 2 classes for binary classification\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "# Create DataLoader for train and validation sets\n",
    "train_dataset = CustomDataset(train_X, train_y, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_X, val_y, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create DataLoader for train and validation sets\n",
    "# train_dataset = CustomDataset(train_df['text'], train_df['binary_label'], tokenizer)\n",
    "# val_dataset = CustomDataset(val_df['text'], val_df['binary_label'], tokenizer)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define training parameters\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eec540f-25ee-4331-8c8a-e150dff8a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = y.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16216c9c-df1c-4369-a7f7-49de36788ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd1e003-3b66-4017-90cf-e6b7e67b6463",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b00a6e-c382-4b45-91c3-0dded40b0693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.44716575883683707, Train Accuracy: 0.7899749373433584, Train Precision: 0.7933771104935861, Train Recall: 0.7899749373433584, Train F1: 0.7873289379332968\n",
      "Val Loss: 0.3019978357478976, Val Accuracy: 0.8637274549098196, Val Precision: 0.8646797569185412, Val Recall: 0.8637274549098196, Val F1: 0.8639803125897617\n",
      "Epoch 2/3, Train Loss: 0.2111232087370895, Train Accuracy: 0.9223057644110275, Train Precision: 0.9225691978479906, Train Recall: 0.9223057644110275, Train F1: 0.9221493456324918\n",
      "Val Loss: 0.3548200121149421, Val Accuracy: 0.8617234468937875, Val Precision: 0.8806230410035034, Val Recall: 0.8617234468937875, Val F1: 0.8621202615032922\n",
      "Epoch 3/3, Train Loss: 0.10847886441837228, Train Accuracy: 0.9664160401002506, Train Precision: 0.9664250985420071, Train Recall: 0.9664160401002506, Train F1: 0.966400697271761\n",
      "Val Loss: 0.248824889305979, Val Accuracy: 0.9038076152304609, Val Precision: 0.9092037060866439, Val Recall: 0.9038076152304609, Val F1: 0.9025614085026711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Initialize empty lists to store loss and accuracy values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_precisions = []\n",
    "val_precisions = []\n",
    "train_recalls = []\n",
    "val_recalls = []\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    correct_train_preds = 0\n",
    "    total_train_preds = 0\n",
    "    train_predicted_labels = []\n",
    "    train_true_labels = []\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct_train_preds += (predicted == labels).sum().item()\n",
    "        total_train_preds += labels.size(0)\n",
    "        train_predicted_labels.extend(predicted.cpu().numpy())\n",
    "        train_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and loss for the epoch\n",
    "    train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train_preds / total_train_preds\n",
    "    train_precision = precision_score(train_true_labels, train_predicted_labels, average='weighted')\n",
    "    train_recall = recall_score(train_true_labels, train_predicted_labels, average='weighted')\n",
    "    train_f1 = f1_score(train_true_labels, train_predicted_labels, average='weighted')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    correct_val_preds = 0\n",
    "    total_val_preds = 0\n",
    "    val_predicted_labels = []\n",
    "    val_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            epoch_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            correct_val_preds += (predicted == labels).sum().item()\n",
    "            total_val_preds += labels.size(0)\n",
    "            val_predicted_labels.extend(predicted.cpu().numpy())\n",
    "            val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and loss for the epoch\n",
    "    val_loss = epoch_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val_preds / total_val_preds\n",
    "    val_precision = precision_score(val_true_labels, val_predicted_labels, average='weighted')\n",
    "    val_recall = recall_score(val_true_labels, val_predicted_labels, average='weighted')\n",
    "    val_f1 = f1_score(val_true_labels, val_predicted_labels, average='weighted')\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1: {train_f1}\")\n",
    "    print(f\"Val Loss: {val_loss}, Val Accuracy: {val_accuracy}, Val Precision: {val_precision}, Val Recall: {val_recall}, Val F1: {val_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c88e58-1699-499d-bac7-c3db2ac6e27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/yadagiri/bert_model/tokenizer_config.json',\n",
       " '/home/yadagiri/bert_model/special_tokens_map.json',\n",
       " '/home/yadagiri/bert_model/vocab.txt',\n",
       " '/home/yadagiri/bert_model/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Initialize the model and tokenizer (assuming you already have them)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"/home/yadagiri/bert_model\")\n",
    "tokenizer.save_pretrained(\"/home/yadagiri/bert_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90bd4eed-0957-475d-840f-6860f342aa4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, f1_score, confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate average test loss and accuracy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m avg_test_loss \u001b[38;5;241m=\u001b[39m test_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader)\n\u001b[1;32m      5\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m correct_preds \u001b[38;5;241m/\u001b[39m total_preds\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate precision, recall, and F1 score\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Calculate average test loss and accuracy\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct_preds / total_preds\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
